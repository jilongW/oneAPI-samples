{
    "guid": "0A887217-5621-4C8D-9418-17558088698B",
    "name": "IntelÂ® Extension for TensorFlow* BF16 Inference",
    "categories": ["Toolkit/oneAPI AI And Analytics/Features and Functionality"],
    "description": "This sample illustrates how to inference a TensorFlow model using Advanced Matrix Extensions Bfloat16",
    "builder": ["cli"],
    "languages": [{"python":{}}],
    "os":["linux"],
    "targetDevice": ["CPU"],
    "cpuInstructionSets": ["AVX512", "AMX"],
    "ciTests": {
      "linux": [
      {
          "env": [
              "pip install uv==0.6.3",
              "uv sync"
          ],
          "id": "intel amx bf16 inference",
          "steps": [
              "uv run jupyter nbconvert --ExecutePreprocessor.enabled=True --to notebook IntelTensorFlow_AMX_BF16_Inference.ipynb"
           ]
      }
       ]
    },
    "expertise": "Code Optimization"
  }
  
