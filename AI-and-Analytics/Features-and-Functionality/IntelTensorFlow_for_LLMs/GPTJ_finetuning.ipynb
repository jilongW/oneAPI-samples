{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-J for GLUE cola dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model : GPT-J (6B)\n",
    " **[GPT-J(6B)] (https://huggingface.co/EleutherAI/gpt-j-6b): released in March 2021.\n",
    "\n",
    "It was the largest open source GPT-3-style language model in the world at the time of release.**\n",
    "\n",
    " **GPT-J is similar to ChatGPT in ability, although it does not function as a chat bot, only as a text predictor.\n",
    "  Developed using Mesh Tranformer & xmap in JAX**\n",
    "\n",
    " *The model consists of :\n",
    ">\n",
    "     - 28 layers\n",
    "     - Model dimension of 4096\n",
    "     - Feedforward dimension of 16384\n",
    "     - 16 heads, each with a dimension of 256.*\n",
    ">\n",
    "*The model is trained with a tokenization vocabulary of 50257, using the same set of Byte Pair Encoding(BPEs) as GPT-2/GPT-3.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset : GLUE cola\n",
    "*CoLA The Corpus of Linguistic Acceptability (Warstadt et al., 2018) consists of English acceptability judgments drawn from books and journal articles on linguistic theory. Each example is a\n",
    "sequence of words annotated with whether it is a grammatical English sentence.*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Finetuning the library models for sequence classification on GLUE.\"\"\"\n",
    "# You can also adapt this script on your own text classification task. Pointers for this are left as comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import python packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import needded classes from HuggingFace transformers library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TFGPTJForSequenceClassification,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version\n",
    "\n",
    "check_min_version(\"4.27.0.dev0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default options for the data.These options can be converted to command line options**\n",
    "*Data, Model & Training options*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataArgs :\n",
    "  def __init__(self):\n",
    "    self.task_name = \"cola\"\n",
    "    self.precision = \"bfloat16\"\n",
    "    self.intra_op_parallelism_threads=56\n",
    "    self.inter_op_parallelism_threads=2\n",
    "    self.max_seq_length=128\n",
    "    self.checkpoint_save_freq = 500\n",
    "    self.overwrite_cache=True\n",
    "    self.max_train_samples=None\n",
    "    self.max_eval_samples=None\n",
    "    self.max_predict_samples=12\n",
    "    self.output_dir =\"./output\"\n",
    "\n",
    "class ModelArgs :\n",
    "  def __init__(self):\n",
    "    self.model_name_or_path = \"EleutherAI/gpt-j-6B\"\n",
    "    self.cache_dir=None\n",
    "    self.model_revision=\"main\"\n",
    "    self.steps=0\n",
    "\n",
    "class TrainingArgs :\n",
    "  def __init__(self):\n",
    "    self.local_rank =-1\n",
    "    self.seed =77\n",
    "    self.num_replicas_in_sync=1\n",
    "    self.per_device_train_batch_size=64\n",
    "    self.per_device_eval_batch_size=64\n",
    "    self.do_train=True\n",
    "    self.do_predict=True\n",
    "    self.do_eval=True\n",
    "    self.num_train_epochs=1.0\n",
    "    self.learning_rate=5e-06\n",
    "    self.output_dir =\"./output\"\n",
    "    self.xla =False\n",
    "\n",
    "\n",
    "data_args = DataArgs()\n",
    "model_args = ModelArgs()\n",
    "training_args = TrainingArgs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set precision and set inter_op and intra op thread settings for best performance\n",
    "\n",
    "*Bfloat16 training gives 2x+ performance compared to fp32 on 4th gen Xeon*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_args.precision == \"bfloat16\" :\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\n",
    "tf.config.threading.set_inter_op_parallelism_threads(data_args.inter_op_parallelism_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(data_args.intra_op_parallelism_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Default Setting for region Logging and transformer verbosity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n",
    "\n",
    "if is_main_process(training_args.local_rank):\n",
    "   transformers.utils.logging.set_verbosity_info()\n",
    "   transformers.utils.logging.enable_default_handler()\n",
    "   transformers.utils.logging.enable_explicit_format()\n",
    "   logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "# endregion\n",
    "\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load the dataset from the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    \"glue\",\n",
    "    data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "\n",
    "#Check the dataset schema and Sample data**\n",
    "\n",
    "print(raw_datasets)\n",
    "print(raw_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model config, tokenizer**\n",
    "**Toekization of dataset : Using gpt2 tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    ")\n",
    "\n",
    "#Load tokenizer for toekization of dataset : Using gpt2 tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"gpt2\" if model_args.model_name_or_path == \"EleutherAI/gpt-j-6B\" else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=True,\n",
    "    revision=model_args.model_revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add special tokens for padding as GPT does not have a padding token.**.\n",
    "*Keys used by tokenizer to select text to be tokenized.\n",
    "Data set used cola.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "}\n",
    "logger = logging.getLogger(__name__)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "config.pad_token_id=0\n",
    "sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
    "\n",
    "#Some models have set the order of the labels to use, so let's make sure we do use it*\n",
    "\n",
    "label_to_id = None\n",
    "config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "print(\"  Label to ID :\", config.label2id)\n",
    "print(\"  ID to Label :\", config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the tokenizer process function. This is called by tokenizer to tokenize relevant data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "       (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=False, max_length=max_seq_length, truncation=True)\n",
    "    return result\n",
    "\n",
    "#Let us no tokenize dataset and set a DataCollator for batching and any padding.\n",
    "\n",
    "datasets = raw_datasets.map(preprocess_function, batched=True, load_from_cache_file=not data_args.overwrite_cache)\n",
    "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few utility fns\n",
    "   >\n",
    "     1. To convert raw dataset to tf_dataset.\n",
    "     2. Number of steps for trainng.\n",
    "     3. Adam optimizer with decay.\n",
    "     4. Call backs for model training.*\n",
    "   >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_Dataset(datasets):\n",
    "    # Convert data to a tf.data.Dataset\n",
    "    dataset_options = tf.data.Options()\n",
    "    dataset_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    num_replicas = -1 #training_args.strategy.num_replicas_in_sync\n",
    "    tf_data = {}\n",
    "    max_samples = {\n",
    "            \"train\": data_args.max_train_samples,\n",
    "            \"validation\": data_args.max_eval_samples,\n",
    "            \"test\": data_args.max_predict_samples,\n",
    "    }\n",
    "    num_replicas=1\n",
    "    for key in datasets.keys():\n",
    "        if key == \"train\" or key.startswith(\"validation\"):\n",
    "            assert \"label\" in datasets[key].features, f\"Missing labels from {key} data!\"\n",
    "        if key == \"train\":\n",
    "            shuffle = True\n",
    "            batch_size = training_args.per_device_train_batch_size * num_replicas\n",
    "        else:\n",
    "            shuffle = False\n",
    "            batch_size = training_args.per_device_eval_batch_size * num_replicas\n",
    "        samples_limit = max_samples[key]\n",
    "        dataset = datasets[key]\n",
    "        if samples_limit is not None:\n",
    "            dataset = dataset.select(range(samples_limit))\n",
    "\n",
    "        # model.prepare_tf_dataset() wraps a Hugging Face dataset in a tf.data.Dataset which is ready to use in\n",
    "        # training. This is the recommended way to use a Hugging Face dataset when training with Keras. You can also\n",
    "        # use the lower-level dataset.to_tf_dataset() method, but you will have to specify things like column names\n",
    "        # yourself if you use this method, whereas they are automatically inferred from the model input names when\n",
    "        # using model.prepare_tf_dataset()\n",
    "        # For more info see the docs:\n",
    "        data = model.prepare_tf_dataset(\n",
    "                dataset,\n",
    "                shuffle=shuffle,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=data_collator,\n",
    "                tokenizer=tokenizer,\n",
    "        )\n",
    "        data = data.with_options(dataset_options)\n",
    "        tf_data[key] = data\n",
    "    return tf_data\n",
    "\n",
    "#Utility fn to compute total number of steps*\n",
    "\n",
    "def compute_num_train_steps(tf_data):\n",
    "    if training_args.do_train:\n",
    "        if model_args.steps:\n",
    "            num_train_steps = model_args.steps\n",
    "            if num_train_steps > int(len(tf_data[\"train\"])) :\n",
    "                # for single epoch\n",
    "                num_train_steps = int(len(tf_data[\"train\"]))\n",
    "        else :\n",
    "            num_train_steps = len(tf_data[\"train\"]) * training_args.num_train_epochs\n",
    "    return num_train_steps\n",
    "\n",
    "#Function to define Adam optimizer with Polynomialdecay*\n",
    "\n",
    "def adam_optimizer_with_decay(num_train_steps):\n",
    "    end_lr = (training_args.learning_rate)/np.sqrt(num_train_steps)\n",
    "    lr_scheduler = PolynomialDecay(\n",
    "        initial_learning_rate=training_args.learning_rate,\n",
    "        end_learning_rate=end_lr, decay_steps=num_train_steps\n",
    "    )\n",
    "    opt = Adam(learning_rate=lr_scheduler)\n",
    "    return opt\n",
    "\n",
    "#Call back for checkpointing if needed*\n",
    "\n",
    "def get_callbacks():\n",
    "    callbacks = []\n",
    "    checkpoint=None\n",
    "    if (checkpoint) :\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "           filepath=training_args.output_dir,\n",
    "           save_weights_only=True,\n",
    "           monitor='accuracy',\n",
    "           mode='max',\n",
    "           save_freq=data_args.checkpoint_save_freq,\n",
    "           save_best_only=True,\n",
    "        )\n",
    "        callbacks.append(checkpoint_callback)\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main steps\n",
    "**Load the model : use model name and config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFGPTJForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw dataset to tf dataset & compile the model**\n",
    "\n",
    "**Get Optimizerand loss. Compile the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data = convert_to_tf_Dataset(datasets)\n",
    "\n",
    "#Get Optimizer,  and loss and compile the model*\n",
    "num_train_steps = compute_num_train_steps(tf_data)\n",
    "optimizer= adam_optimizer_with_decay(num_train_steps)\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"], jit_compile=training_args.xla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model : Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks= get_callbacks()\n",
    "steps_pe = int(len(tf_data[\"train\"]))\n",
    "model.fit(\n",
    "    tf_data[\"train\"],\n",
    "    validation_data=tf_data[\"validation\"],\n",
    "    epochs=int(training_args.num_train_epochs),\n",
    "    steps_per_epoch=steps_pe,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us save and reload the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if training_args.output_dir :\n",
    "    # If we're not pushing to hub, at least save a local copy when we're done\n",
    "    print(\"Save the model id dir :\",training_args.output_dir)\n",
    "    model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us check some classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results for test\n",
    "# Show results for test\n",
    "metric = evaluate.load(\"glue\", data_args.task_name)\n",
    "def show_results(class_preds, key):\n",
    "    for i in range(7):\n",
    "      pred = int(class_preds[i])\n",
    "      pred_label = config.id2label[pred]\n",
    "      if data_args.task_name != 'mrpc':\n",
    "        print(f\"Sentence : {raw_datasets[key][i]['sentence']} : {pred_label}\")\n",
    "      else:\n",
    "        sent = raw_datasets[key][i]['sentence1'] + \" : \" + raw_datasets[key][i]['sentence2']\n",
    "        print(f\"Sentences : {sent} : {pred_label}\")\n",
    "\n",
    "def val_predict(model, tf_data, key):\n",
    "    print(\"====================\",key, \"=========================\")\n",
    "    preds = model.predict(tf_data[key])[\"logits\"]\n",
    "    print(\" Done predictions:..\")\n",
    "    class_preds = tf.math.argmax(preds, axis=1)\n",
    "    if key != \"test\":\n",
    "      print(f\"{key} Accuracy :\", accuracy_score(class_preds,raw_datasets[key][\"label\"]))\n",
    "      print(metric.compute(predictions=class_preds, references=raw_datasets[key][\"label\"]))\n",
    "    else :\n",
    "      show_results(class_preds, key)\n",
    "    print(\"===================\", key, \" done.==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predict(model, tf_data, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.output_dir :\n",
    "    # If we're not pushing to hub, at least save a local copy when we're done\n",
    "    print(\"Save the model id dir :\",training_args.output_dir)\n",
    "    model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[CODE_SAMPLE_COMPLETED_SUCCESSFULLY]\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
